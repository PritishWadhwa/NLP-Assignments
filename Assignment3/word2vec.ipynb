{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python import keras\n",
    "from keras import utils as np_util\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.math import argmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './../data/Brown_tagged_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData = train_test_split(data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(test_Xy):\n",
    "    \"\"\"\n",
    "    test_Xy: List of list of tokens and tags\n",
    "    Returns: List of tokens and list of tags\n",
    "    \"\"\"\n",
    "    test_y = []\n",
    "    test_X = []\n",
    "\n",
    "    for sent in test_Xy:\n",
    "        tagged_sent = sent.split(' ')\n",
    "        sent_y = []\n",
    "        sent_X = []\n",
    "\n",
    "        for word in tagged_sent:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            sent_X.append(actual_word)\n",
    "            sent_y.append(tag)\n",
    "\n",
    "        test_y.append(sent_y)\n",
    "        test_X.append(sent_X)\n",
    "\n",
    "    return test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_word(inp):\n",
    "    \"\"\"\n",
    "    Returns word, tag for the given input\n",
    "    \"\"\"\n",
    "    arr = inp.split('/')\n",
    "    tag = arr[-1]\n",
    "    del arr[-1]\n",
    "    word = '/'.join(arr)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSents, trainSentTags = split_Xy(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSents, testSentTags = split_Xy(validData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(trainSents, trainSentTags):\n",
    "    uniqueWords = set()\n",
    "    for sent in trainSents:\n",
    "        for word in sent:\n",
    "            uniqueWords.add(word)\n",
    "\n",
    "    uniqueTags = set()\n",
    "    for sent in trainSentTags:\n",
    "        for tag in sent:\n",
    "            uniqueTags.add(tag)\n",
    "\n",
    "    tagIndex = {}\n",
    "    idx = 0\n",
    "    for i in uniqueTags:\n",
    "        tagIndex[i] = idx\n",
    "        idx += 1\n",
    "\n",
    "    wordIndex = {}\n",
    "    idx = 0\n",
    "    for i in uniqueWords:\n",
    "        wordIndex[i] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # trainSentsText = trainSents.copy()\n",
    "    # trainTagsText = trainSentTags.copy()\n",
    "    trainSents[0]\n",
    "    wordTokenize = Tokenizer()\n",
    "    wordTokenize.fit_on_texts(trainSents)\n",
    "    trainSents = wordTokenize.texts_to_sequences(trainSents)\n",
    "    paddedTrainSents = pad_sequences(\n",
    "        trainSents, maxlen=300, padding='pre', truncating='post')\n",
    "    tagTokenize = Tokenizer()\n",
    "    tagTokenize.fit_on_texts(trainSentTags)\n",
    "    trainSentTags = tagTokenize.texts_to_sequences(trainSentTags)\n",
    "    paddedTrainSentTags = pad_sequences(trainSentTags, maxlen=300, padding='pre', truncating='post')\n",
    "\n",
    "    oneHotEncodedTrainSentTags = to_categorical(paddedTrainSentTags, num_classes=None, dtype='float32')\n",
    "\n",
    "    return paddedTrainSents, oneHotEncodedTrainSentTags, tagIndex, wordIndex, trainSents, wordTokenize, tagTokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPaddedSents, trainOneHotEncodedTags, trainTagIndex, trainWordIndex, trainSents, trainWordTokenizer, tagWordTokenize = preprocessData(trainSents, trainSentTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPaddedSents, testOneHotEncodedTags, testTagIndex, testWordIndex, testSents, testWordTokenizer, testTagWordTokenizer = preprocessData(testSents, testSentTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vgensim = gensim.models.word2vec.Word2Vec(trainSents, vector_size=300, min_count=1, window=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(trainWordTokenizer.word_index) + 1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndices = trainWordTokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in wordIndices.items():\n",
    "    try:\n",
    "        embeddings[index, :] = w2vgensim.wv.get_vector(trainWordTokenizer.word_index[word])\n",
    "    except:\n",
    "        embeddings[index, :] = np.random.uniform(-0.25, 0.25, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 300)          8256000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300, 100)          30100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300, 13)           1313      \n",
      "=================================================================\n",
      "Total params: 8,287,413\n",
      "Trainable params: 8,287,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = keras.Sequential()\n",
    "mlp.add(keras.layers.Embedding(len(trainWordTokenizer.word_index) + 1, 300, weights=[embeddings], input_length=300, trainable=True))\n",
    "mlp.add((keras.layers.Dense(100, activation='relu')))\n",
    "mlp.add((keras.layers.Dense(13, activation='relu')))\n",
    "mlp.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 94s 530ms/step - loss: 0.0747 - accuracy: 0.9150 - val_loss: 0.0765 - val_accuracy: 0.9579\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 96s 556ms/step - loss: 0.0737 - accuracy: 0.9144 - val_loss: 0.0764 - val_accuracy: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4a0dc5460>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(trainPaddedSents, trainOneHotEncodedTags, batch_size=128,\n",
    "          epochs=2, validation_data=(testPaddedSents, testOneHotEncodedTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
