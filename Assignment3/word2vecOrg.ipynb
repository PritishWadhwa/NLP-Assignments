{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python import keras\n",
    "from keras import utils as np_util\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.math import argmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './../data/Brown_tagged_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData = train_test_split(data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(test_Xy):\n",
    "    \"\"\"\n",
    "    test_Xy: List of list of tokens and tags\n",
    "    Returns: List of tokens and list of tags\n",
    "    \"\"\"\n",
    "    test_y = []\n",
    "    test_X = []\n",
    "\n",
    "    for sent in test_Xy:\n",
    "        tagged_sent = sent.split(' ')\n",
    "        sent_y = []\n",
    "        sent_X = []\n",
    "\n",
    "        for word in tagged_sent:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            sent_X.append(actual_word)\n",
    "            sent_y.append(tag)\n",
    "\n",
    "        test_y.append(sent_y)\n",
    "        test_X.append(sent_X)\n",
    "\n",
    "    return test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_word(inp):\n",
    "    \"\"\"\n",
    "    Returns word, tag for the given input\n",
    "    \"\"\"\n",
    "    arr = inp.split('/')\n",
    "    tag = arr[-1]\n",
    "    del arr[-1]\n",
    "    word = '/'.join(arr)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSents, trainSentTags = split_Xy(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSents, testSentTags = split_Xy(validData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(trainSents, trainSentTags):\n",
    "    uniqueWords = set()\n",
    "    for sent in trainSents:\n",
    "        for word in sent:\n",
    "            uniqueWords.add(word)\n",
    "\n",
    "    uniqueTags = set()\n",
    "    for sent in trainSentTags:\n",
    "        for tag in sent:\n",
    "            uniqueTags.add(tag)\n",
    "\n",
    "    tagIndex = {}\n",
    "    idx = 0\n",
    "    for i in uniqueTags:\n",
    "        tagIndex[i] = idx\n",
    "        idx += 1\n",
    "\n",
    "    wordIndex = {}\n",
    "    idx = 0\n",
    "    for i in uniqueWords:\n",
    "        wordIndex[i] = idx\n",
    "        idx += 1\n",
    "\n",
    "    # trainSentsText = trainSents.copy()\n",
    "    # trainTagsText = trainSentTags.copy()\n",
    "    trainSents[0]\n",
    "    wordTokenize = Tokenizer()\n",
    "    wordTokenize.fit_on_texts(trainSents)\n",
    "    trainSents = wordTokenize.texts_to_sequences(trainSents)\n",
    "    paddedTrainSents = pad_sequences(\n",
    "        trainSents, maxlen=387, padding='pre', truncating='post')\n",
    "    tagTokenize = Tokenizer()\n",
    "    tagTokenize.fit_on_texts(trainSentTags)\n",
    "    trainSentTags = tagTokenize.texts_to_sequences(trainSentTags)\n",
    "    paddedTrainSentTags = pad_sequences(trainSentTags, maxlen=387, padding='pre', truncating='post')\n",
    "\n",
    "    oneHotEncodedTrainSentTags = to_categorical(paddedTrainSentTags, num_classes=None, dtype='float32')\n",
    "\n",
    "    return paddedTrainSents, oneHotEncodedTrainSentTags, tagIndex, wordIndex, trainSents, wordTokenize, tagTokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingPath = './../data/GoogleNews-vectors-negative300.bin'\n",
    "embeddingsw2v = KeyedVectors.load_word2vec_format(embeddingPath, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel(trainSents, trainSentTags, testSents, testSentTags):\n",
    "    trainPaddedSents, trainOneHotEncodedTags, trainTagIndex, trainWordIndex, trainSents, trainWordTokenizer, traintagWordTokenizer = preprocessData(\n",
    "        trainSents, trainSentTags)\n",
    "    testPaddedSents, testOneHotEncodedTags, testTagIndex, testWordIndex, testSents, testWordTokenizer, testTagWordTokenizer = preprocessData(\n",
    "        testSents, testSentTags)\n",
    "    w2vgensim = gensim.models.word2vec.Word2Vec(\n",
    "        trainSents, vector_size=300, min_count=1, window=5)\n",
    "    embeddings = np.zeros((len(trainWordTokenizer.word_index) + 1, 300))\n",
    "    wordIndices = trainWordTokenizer.word_index\n",
    "    for word, index in wordIndices.items():\n",
    "        try:\n",
    "            embeddings[index, :] = embeddingsw2v[word]\n",
    "        except:\n",
    "            embeddings[index, :] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    \n",
    "    mlp = keras.Sequential()\n",
    "    mlp.add(keras.layers.Embedding(len(trainWordTokenizer.word_index) + 1,\n",
    "                                300, weights=[embeddings], input_length=387, trainable=True))\n",
    "    mlp.add((keras.layers.Dense(100, activation='relu')))\n",
    "    mlp.add((keras.layers.Dense(13, activation='softmax')))\n",
    "    mlp.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    mlp.summary()\n",
    "    mlp.fit(trainPaddedSents, trainOneHotEncodedTags, batch_size=128,\n",
    "            epochs=2, validation_data=(testPaddedSents, testOneHotEncodedTags))\n",
    "    return mlp, traintagWordTokenizer, testTagWordTokenizer, trainPaddedSents, trainOneHotEncodedTags, testPaddedSents, testOneHotEncodedTags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 387, 300)          8256000   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 387, 100)          30100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 387, 13)           1313      \n",
      "=================================================================\n",
      "Total params: 8,287,413\n",
      "Trainable params: 8,287,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "172/172 [==============================] - 65s 369ms/step - loss: 0.0141 - accuracy: 0.9791 - val_loss: 0.0033 - val_accuracy: 0.9728\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 65s 379ms/step - loss: 4.6201e-04 - accuracy: 0.9960 - val_loss: 0.0038 - val_accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "mlp, trainTagWordTokenizer, testTagWordTokenizer, trainFinalSents, trainFinalTags, testFinalSents, testFinalTags = makeModel(trainSents, trainSentTags, testSents, testSentTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 387) for input KerasTensor(type_spec=TensorSpec(shape=(None, 387), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.predict_classes(testFinalSents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverseTagMapTest = dict(map(reversed, testTagWordTokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'noun',\n",
       " 2: 'verb',\n",
       " 3: '.',\n",
       " 4: 'adp',\n",
       " 5: 'det',\n",
       " 6: 'adj',\n",
       " 7: 'adv',\n",
       " 8: 'pron',\n",
       " 9: 'conj',\n",
       " 10: 'prt',\n",
       " 11: 'num',\n",
       " 12: 'x'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseTagMapTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverseTagMapTest[0] = 'pad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'noun',\n",
       " 2: 'verb',\n",
       " 3: '.',\n",
       " 4: 'adp',\n",
       " 5: 'det',\n",
       " 6: 'adj',\n",
       " 7: 'adv',\n",
       " 8: 'pron',\n",
       " 9: 'conj',\n",
       " 10: 'prt',\n",
       " 11: 'num',\n",
       " 12: 'x',\n",
       " 0: 'pad'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseTagMapTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reshape(pred.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [reverseTagMapTest[i] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'verb',\n",
       " 'adv',\n",
       " 'prt',\n",
       " 'adp',\n",
       " 'det',\n",
       " 'adv',\n",
       " 'verb',\n",
       " '.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = testFinalTags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [np.argmax(i) for i in expected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [reverseTagMapTest[i] for i in expected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'pad',\n",
       " 'noun',\n",
       " 'verb',\n",
       " 'adv',\n",
       " 'adp',\n",
       " 'det',\n",
       " 'adj',\n",
       " 'noun',\n",
       " '.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
