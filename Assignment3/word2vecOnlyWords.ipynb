{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveEmbeddings = {}\n",
    "f = open('../data/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    gloveEmbeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './../data/Brown_tagged_train.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(test_Xy):\n",
    "    \"\"\"\n",
    "    test_Xy: List of list of tokens and tags\n",
    "    Returns: List of tokens and list of tags\n",
    "    \"\"\"\n",
    "    test_y = []\n",
    "    test_X = []\n",
    "\n",
    "    for sent in test_Xy:\n",
    "        tagged_sent = sent.split(' ')\n",
    "        sent_y = []\n",
    "        sent_X = []\n",
    "\n",
    "        for word in tagged_sent:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            sent_X.append(actual_word)\n",
    "            sent_y.append(tag)\n",
    "\n",
    "        test_y.append(sent_y)\n",
    "        test_X.append(sent_X)\n",
    "\n",
    "    return test_X, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_word(inp):\n",
    "    \"\"\"\n",
    "    Returns word, tag for the given input\n",
    "    \"\"\"\n",
    "    arr = inp.split('/')\n",
    "    tag = arr[-1]\n",
    "    del arr[-1]\n",
    "    word = '/'.join(arr)\n",
    "    return word, tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testXy = split_Xy(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(sentences, tags, window=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    unk = 0\n",
    "    for ctr in tqdm(range(len(sentences))):\n",
    "        sent = sentences[ctr]\n",
    "        for i in range(len(sent)):\n",
    "            vec, unknown = features_embs(sent, i, window)\n",
    "            X.append(vec)\n",
    "            y.append(tags[ctr][i])\n",
    "            unk += unknown\n",
    "    return X, y, unk\n",
    "\n",
    "\n",
    "def vectorize(trainSent, trainTags, window=1, embedding='word2vec'):\n",
    "    X, y, unk = transform_to_dataset(trainSent, trainTags, window)\n",
    "    # X = np.array(X)\n",
    "    # y = np.array(y)\n",
    "    print('Unknown words:', unk)\n",
    "    X = np.asarray(X, dtype=object)\n",
    "    y = np.asarray(y, dtype=object)\n",
    "    print('X shape:', X.shape)\n",
    "    print('y shape:', y.shape)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def getYtrain(sentences, tags):\n",
    "    y_train = []\n",
    "    for ctr in tqdm(range(len(sentences))):\n",
    "        sent = sentences[ctr]\n",
    "        for i in range(len(sent)):\n",
    "            y_train.append(tags[ctr][i])\n",
    "    y_train = np.array(y_train)\n",
    "    return y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27491/27491 [00:00<00:00, 95873.97it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train = getYtrain(trainSet, testXy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543149, 12)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27491/27491 [00:22<00:00, 1205.76it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "out_ = 0\n",
    "in_ = 0\n",
    "for i in tqdm(range(len(trainSet))):\n",
    "    for j in trainSet[i]:\n",
    "        try:\n",
    "            embs.append(embeddings[j])\n",
    "            in_ += 1\n",
    "        except:\n",
    "            oov = np.random.uniform(-0.25, 0.25, 300)\n",
    "            embs.append(oov)\n",
    "            out_ += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous:  543149 (543149, 12)\n",
      "Current:  434519 108630 (434519, 12) (108630, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "embs_train1, embs_test1, y_train1, y_test1 = train_test_split(\n",
    "    embs, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Previous: \", len(embs), y_train.shape)\n",
    "print(\"Current: \", len(embs_train1), len(\n",
    "    embs_test1), y_train1.shape, y_test1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_train1 = np.array(embs_train1)\n",
    "embs_test1 = np.array(embs_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 31,312\n",
      "Trainable params: 31,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=embs_train1.shape[1]))\n",
    "# model.add(Dense(300, activation='relu'))\n",
    "# model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(y_train1.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 22:37:19.699491: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-31 22:37:19.699525: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-31 22:37:19.700341: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3395/3395 [==============================] - 15s 4ms/step - loss: 2.1754 - accuracy: 0.2202 - val_loss: 2.1657 - val_accuracy: 0.2228\n",
      "Epoch 2/7\n",
      "3395/3395 [==============================] - 15s 4ms/step - loss: 2.1626 - accuracy: 0.2221 - val_loss: 2.1667 - val_accuracy: 0.2226\n",
      "Epoch 3/7\n",
      "3395/3395 [==============================] - 14s 4ms/step - loss: 2.1580 - accuracy: 0.2223 - val_loss: 2.1687 - val_accuracy: 0.2216\n",
      "Epoch 4/7\n",
      "3395/3395 [==============================] - 15s 4ms/step - loss: 2.1526 - accuracy: 0.2229 - val_loss: 2.1723 - val_accuracy: 0.2209\n",
      "Epoch 5/7\n",
      "3395/3395 [==============================] - 14s 4ms/step - loss: 2.1472 - accuracy: 0.2236 - val_loss: 2.1761 - val_accuracy: 0.2180\n",
      "Epoch 6/7\n",
      "3395/3395 [==============================] - 15s 5ms/step - loss: 2.1426 - accuracy: 0.2250 - val_loss: 2.1801 - val_accuracy: 0.2145\n",
      "Epoch 7/7\n",
      "3395/3395 [==============================] - 14s 4ms/step - loss: 2.1385 - accuracy: 0.2256 - val_loss: 2.1837 - val_accuracy: 0.2159\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 7\n",
    "batch_size = 128\n",
    "cp = ModelCheckpoint(filepath=\"tagger.h5\",\n",
    "                     monitor='val_acc',\n",
    "                     save_best_only=True,\n",
    "                     verbose=1)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 write_graph=True,\n",
    "                 write_images=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "history = model.fit(embs_train1, y_train1,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1, validation_data=(embs_test1, y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
