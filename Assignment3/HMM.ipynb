{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'Brown_tagged_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "# data = map(lambda x: '<s>/<s> ' + x, data)\n",
    "data = np.array(list(data))\n",
    "unique_tags = ['ADP', 'DET', 'NOUN', 'VERB', 'ADJ', 'CONJ', 'PRT', '.', 'ADV', 'NUM', 'PRON', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(test_Xy):\n",
    "    \"\"\"\n",
    "    test_Xy: List of list of tokens and tags\n",
    "    Returns: List of tokens and list of tags\n",
    "    \"\"\"\n",
    "    test_y = []\n",
    "    test_X = []\n",
    "\n",
    "    for sent in test_Xy:\n",
    "        tagged_sent = sent.split(' ')\n",
    "        sent_y = []\n",
    "        sent_X = []\n",
    "\n",
    "        for word in tagged_sent:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            sent_X.append(actual_word)\n",
    "            sent_y.append(tag)\n",
    "            \n",
    "        test_y.append(sent_y)\n",
    "        test_X.append(sent_X)\n",
    "\n",
    "    return test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_word(inp):\n",
    "    \"\"\"\n",
    "    Returns word, tag for the given input\n",
    "    \"\"\"\n",
    "    arr = inp.split('/')\n",
    "    tag = arr[-1]\n",
    "    del arr[-1]\n",
    "    word = '/'.join(arr)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(train_set):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of parameters\n",
    "    tokens = Tokens present in the vocabulary\n",
    "    tags = Tags present in the vocabulary\n",
    "    token_tag = Count of each word/tag pair for emmission probability\n",
    "    tag_pair = Count of each tag pair for transition probability\n",
    "    vocabulary = Vocabulary of the training set\n",
    "    Create params for HMM\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    for sent in train_set:\n",
    "        tagged_sentences.append(sent.split(' '))\n",
    "\n",
    "    # list of list of tokens in the training set. Each internal set denotes a sentence.\n",
    "    tokens = []\n",
    "    tag_freq = {}  # list of all tags\n",
    "    token_tag = {}  # dictionary of token/tag\n",
    "    tag_pair = {}  # dictionary of tag tag\n",
    "    vocab = set()\n",
    "\n",
    "    # Create vocab, tokens, tags, token_tag\n",
    "    for sentence in tagged_sentences:\n",
    "        sentence_tokens = []  # all tokens for a sentence\n",
    "        for word in sentence:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            token_tag[word] = token_tag.get(word, 0) + 1\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            tag_freq[tag] = tag_freq.get(tag, 0) + 1\n",
    "            vocab.add(actual_word)\n",
    "            sentence_tokens.append(actual_word)\n",
    "        tokens.append(sentence_tokens)\n",
    "\n",
    "    # Create tag_pair\n",
    "    for sentence in tagged_sentences:\n",
    "        tag_bigram = bigrams(sentence)\n",
    "        for t in tag_bigram:\n",
    "            if t[0] == '' or t[1] == '':\n",
    "                continue\n",
    "            tag_1 = split_tag_word(t[0])[1]\n",
    "            tag_2 = split_tag_word(t[1])[1]\n",
    "            key = tag_1 + ' ' + tag_2\n",
    "            tag_pair[key] = tag_pair.get(key, 0) + 1\n",
    "\n",
    "    print(\"Vocabulary Length: \", len(vocab))\n",
    "    print(\"Number of Tags: \", len(tag_freq.keys()))\n",
    "    print(tag_freq)\n",
    "    print(\"Number of Token/Tag: \", len(token_tag.keys()))\n",
    "    print(\"Number of Tag Tag: \", len(tag_pair.keys()))\n",
    "    return {\n",
    "        \"vocab\": vocab,\n",
    "        \"tokens\": tokens,\n",
    "        \"tag_freq\": tag_freq,\n",
    "        \"token_tag\": token_tag,\n",
    "        \"tag_pair\": tag_pair\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(params, sentence):\n",
    "    \"\"\"\n",
    "    Takes a test sentence and returns predictions\n",
    "    \"\"\"\n",
    "    vocab = params['vocab']\n",
    "    tag_freq = params['tag_freq']\n",
    "    tags = list(tag_freq.keys())\n",
    "    token_tag = params['token_tag']\n",
    "    tag_pair = params['tag_pair']\n",
    "    prev_tag = '.'\n",
    "    chosen_tags = []\n",
    "\n",
    "    for word in sentence:\n",
    "        count_emiss = True\n",
    "\n",
    "        if word == \"\" or word not in vocab:\n",
    "            count_emiss = False\n",
    "            # chosen_tags.append('.')\n",
    "            # In case of OOV words, choose the tag with the highest probability, Basically only the transition probability\n",
    "            # print(\"Invalid token found: \", word)\n",
    "            # continue\n",
    "        probs = []\n",
    "\n",
    "        for tag in tags:\n",
    "            # Transmission probability\n",
    "            p_trans = tag_pair.get(prev_tag + ' ' + tag,\n",
    "                                   0) / tag_freq.get(prev_tag, -1)\n",
    "            if count_emiss:\n",
    "                # Emmission probability (only counted if the word is not OOV)\n",
    "                p_emiss = token_tag.get(\n",
    "                    word + \"/\" + tag, 0) / tag_freq.get(tag, -1)\n",
    "                probs.append(p_trans * p_emiss)\n",
    "            else:\n",
    "                probs.append(p_trans)\n",
    "\n",
    "        chosen_tag = tags[np.argmax(probs)]\n",
    "        prev_tag = chosen_tag\n",
    "        chosen_tags.append(chosen_tag)\n",
    "\n",
    "    return chosen_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM(train, test, create_params, viterbi):\n",
    "    \"\"\"\n",
    "    Takes trainset and test sentences and returns predictions\n",
    "    \"\"\"\n",
    "    params = create_params(train)\n",
    "\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "        sentence = test[i]\n",
    "        predicted_tags = viterbi(params, sentence)\n",
    "        preds.append(predicted_tags)\n",
    "\n",
    "    tag_freq = params['tag_freq']\n",
    "    tags = list(tag_freq.keys())\n",
    "\n",
    "    return tags, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_HMM(tags, preds, test_y, to_print=True):\n",
    "    \"\"\"\n",
    "    tags: List of tags\n",
    "    preds: List of list of predicted tags\n",
    "    test_y: List of list of actual tags\n",
    "    \"\"\"\n",
    "    # for i in range(len(test_y)):\n",
    "    #     for j in range(len(test_y[i])):\n",
    "\n",
    "    flat_tags = [item for elem in test_y for item in elem]\n",
    "    flat_preds = [item for elem in preds for item in elem]\n",
    "\n",
    "    accuracy = accuracy_score(flat_tags, flat_preds)\n",
    "    precision = precision_score(flat_tags, flat_preds, average='weighted')\n",
    "    recall = recall_score(flat_tags, flat_preds, average='weighted')\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    tagwise_precision = precision_score(\n",
    "        flat_tags, flat_preds, labels=tags, pos_label=None, average=None)\n",
    "    tagwise_recall = recall_score(\n",
    "        flat_tags, flat_preds, labels=tags, pos_label=None, average=None)\n",
    "    tagwise_f1 = (2 * tagwise_precision * tagwise_recall) / \\\n",
    "        (tagwise_precision + tagwise_recall)\n",
    "\n",
    "    cm = confusion_matrix(flat_tags, flat_preds)\n",
    "\n",
    "    # cm = np.zeros((len(tags), len(tags)))\n",
    "\n",
    "    # for sent_y, sent_pred in zip(test_y, preds):\n",
    "    #     for tag, pred in zip(sent_y, sent_pred):\n",
    "    #         cm[tags.index(tag)][tags.index(pred)] += 1\n",
    "\n",
    "    if to_print:\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 score\", f1_score)\n",
    "        print(\"Tagwise Precision: \", tagwise_precision)\n",
    "        print(\"Tagwise Recall: \", tagwise_recall)\n",
    "        print(\"Tagwise F1 score: \", tagwise_f1)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tags)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1_score,\n",
    "        \"cm\": cm,\n",
    "        \"tagwise_precision\": tagwise_precision,\n",
    "        \"tagwise_recall\": tagwise_recall,\n",
    "        \"tagwise_f1\": tagwise_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldHMM(data, create_params, viterbi):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    unique_tags = []\n",
    "    metrics = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train_set, test_Xy = data[train_index], data[test_index]\n",
    "        test_X, test_y = split_Xy(test_Xy)\n",
    "        tags, preds = HMM(train_set, test_X, create_params, viterbi)\n",
    "        unique_tags = tags\n",
    "        metrics.append(evaluate_HMM(tags, preds, test_y, to_print=False))\n",
    "\n",
    "    print(\"Average metrics: \")\n",
    "    precision, recall, f1, accuracy = 0, 0, 0, 0\n",
    "    tagwise_precision, tagwise_recall, tagwise_f1 = np.zeros(\n",
    "        len(unique_tags)), np.zeros(len(unique_tags)), np.zeros(len(unique_tags))\n",
    "\n",
    "    cm = np.zeros((len(unique_tags), len(unique_tags)))\n",
    "\n",
    "    for metric_dict in metrics:\n",
    "        accuracy += metric_dict['accuracy']\n",
    "        precision += metric_dict[\"precision\"]\n",
    "        recall += metric_dict[\"recall\"]\n",
    "        f1 += metric_dict[\"f1-score\"]\n",
    "        tagwise_precision += metric_dict[\"tagwise_precision\"]\n",
    "        tagwise_recall += metric_dict[\"tagwise_recall\"]\n",
    "        tagwise_f1 += metric_dict[\"tagwise_f1\"]\n",
    "        cm += metric_dict[\"cm\"]\n",
    "\n",
    "    accuracy /= len(metrics)\n",
    "    precision /= len(metrics)\n",
    "    recall /= len(metrics)\n",
    "    f1 /= len(metrics)\n",
    "    tagwise_precision /= len(metrics)\n",
    "    tagwise_recall /= len(metrics)\n",
    "    tagwise_f1 /= len(metrics)\n",
    "    cm /= len(metrics)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : f1,\n",
    "        \"tagwise_precision\" : tagwise_precision,\n",
    "        \"tagwise_recall\" : tagwise_recall,\n",
    "        \"tagwise_f1\" : tagwise_f1,\n",
    "        \"cm\" : cm\n",
    "    }\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    # print(\"F1 score: \", f1)\n",
    "    # # create df of tagwise metrics\n",
    "    # tagwise_metrics = pd.DataFrame(\n",
    "    #     {\n",
    "    #         \"tag\": unique_tags,\n",
    "    #         \"precision\": tagwise_precision,\n",
    "    #         \"recall\": tagwise_recall,\n",
    "    #         \"f1-score\": tagwise_f1\n",
    "    #     }\n",
    "    # )\n",
    "    # print(tagwise_metrics)\n",
    "    # disp = ConfusionMatrixDisplay(\n",
    "    #     confusion_matrix=cm, display_labels=unique_tags)\n",
    "    # disp.plot()\n",
    "    # fig = disp.figure_\n",
    "    # fig.set_figwidth(12)\n",
    "    # fig.set_figheight(12)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27813\n",
      "Number of Tags:  12\n",
      "{'ADP': 43660, 'DET': 42087, 'NOUN': 80317, 'VERB': 59540, 'ADJ': 24131, 'CONJ': 11867, 'PRT': 10219, '.': 47569, 'ADV': 18429, 'NUM': 4360, 'PRON': 18010, 'X': 398}\n",
      "Number of Token/Tag:  29593\n",
      "Number of Tag Tag:  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9164/9164 [00:03<00:00, 2907.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27702\n",
      "Number of Tags:  12\n",
      "{'VERB': 60045, 'ADJ': 24352, 'NOUN': 80458, 'DET': 42158, 'ADP': 43855, '.': 47682, 'CONJ': 11875, 'ADV': 18457, 'PRON': 18081, 'PRT': 10510, 'NUM': 4454, 'X': 398}\n",
      "Number of Token/Tag:  29491\n",
      "Number of Tag Tag:  141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9164/9164 [00:04<00:00, 2267.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27748\n",
      "Number of Tags:  12\n",
      "{'ADP': 44207, 'DET': 42083, 'NOUN': 80765, 'VERB': 59947, 'ADJ': 24449, 'CONJ': 11816, 'PRT': 10515, '.': 47859, 'ADV': 18480, 'NUM': 4514, 'PRON': 18367, 'X': 384}\n",
      "Number of Token/Tag:  29544\n",
      "Number of Tag Tag:  141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9163/9163 [00:03<00:00, 2316.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average metrics: \n"
     ]
    }
   ],
   "source": [
    "q1a_metrics = KFoldHMM(data, create_params, viterbi)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission Probabilities\n",
    "\n",
    "P(word | state) = count(word, state) / count(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probabilities\n",
    "\n",
    "P(state | state-1) = count(state, state-1) / count(state-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Length 2 Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(train_set):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of parameters\n",
    "    tokens = Tokens present in the vocabulary\n",
    "    tags = Tags present in the vocabulary\n",
    "    token_tag = Count of each word/tag pair for emmission probability\n",
    "    tag_pair = Count of each tag pair for transition probability\n",
    "    vocabulary = Vocabulary of the training set\n",
    "    Create params for HMM\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    for sent in train_set:\n",
    "        tagged_sentences.append(sent.split(' '))\n",
    "\n",
    "    # list of list of tokens in the training set. Each internal set denotes a sentence.\n",
    "    tokens = []\n",
    "    tag_freq = {}  # list of all tags\n",
    "    token_tag = {}  # dictionary of token/tag\n",
    "    tag_pair = {}  # dictionary of tag tag\n",
    "    tag_triplet = {}  # dictionary of tag tag\n",
    "    vocab = set()\n",
    "\n",
    "    # Create vocab, tokens, tags, token_tag\n",
    "    for sentence in tagged_sentences:\n",
    "        sentence_tokens = []  # all tokens for a sentence\n",
    "        for word in sentence:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            token_tag[word] = token_tag.get(word, 0) + 1\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            tag_freq[tag] = tag_freq.get(tag, 0) + 1\n",
    "            vocab.add(actual_word)\n",
    "            sentence_tokens.append(actual_word)\n",
    "        tokens.append(sentence_tokens)\n",
    "\n",
    "    # Create tag_pair\n",
    "    for sentence in tagged_sentences:\n",
    "        tag_bigram = bigrams(sentence)\n",
    "        for t in tag_bigram:\n",
    "            if t[0] == '' or t[1] == '':\n",
    "                continue\n",
    "            tag_1 = split_tag_word(t[0])[1]\n",
    "            tag_2 = split_tag_word(t[1])[1]\n",
    "            key = tag_1 + ' ' + tag_2\n",
    "            tag_pair[key] = tag_pair.get(key, 0) + 1\n",
    "        tag_trigram = trigrams(sentence)\n",
    "        for t in tag_trigram:\n",
    "            if t[0] == '' or t[1] == '' or t[2] == '':\n",
    "                continue\n",
    "            tag_1 = split_tag_word(t[0])[1]\n",
    "            tag_2 = split_tag_word(t[1])[1]\n",
    "            tag_3 = split_tag_word(t[2])[1]\n",
    "            key = tag_1 + ' ' + tag_2 + ' ' + tag_3\n",
    "            tag_triplet[key] = tag_triplet.get(key, 0) + 1\n",
    "\n",
    "    print(\"Vocabulary Length: \", len(vocab))\n",
    "    print(\"Number of Tags: \", len(tag_freq.keys()))\n",
    "    print(tag_freq)\n",
    "    print(\"Number of Token/Tag: \", len(token_tag.keys()))\n",
    "    print(\"Number of Tag Tag: \", len(tag_pair.keys()))\n",
    "    print(\"Number of Tag Tag Tag: \", len(tag_triplet.keys()))\n",
    "    return {\n",
    "        \"vocab\": vocab,\n",
    "        \"tokens\": tokens,\n",
    "        \"tag_freq\": tag_freq,\n",
    "        \"token_tag\": token_tag,\n",
    "        \"tag_pair\": tag_pair,\n",
    "        \"tag_triplet\": tag_triplet\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(params, sentence):\n",
    "    \"\"\"\n",
    "    Takes a test sentence and returns predictions\n",
    "    \"\"\"\n",
    "    vocab = params['vocab']\n",
    "    tag_freq = params['tag_freq']\n",
    "    tags = list(tag_freq.keys())\n",
    "    token_tag = params['token_tag']\n",
    "    tag_pair = params['tag_pair']\n",
    "    tag_triplet = params['tag_triplet']\n",
    "    prev_tag = '.'\n",
    "    prev2_tag = '.'\n",
    "    chosen_tags = []\n",
    "\n",
    "    for word in sentence:\n",
    "        count_emiss = True\n",
    "\n",
    "        if word == \"\" or word not in vocab:\n",
    "            count_emiss = False\n",
    "            # chosen_tags.append('.')\n",
    "            # In case of OOV words, choose the tag with the highest probability, Basically only the transition probability\n",
    "            # print(\"Invalid token found: \", word)\n",
    "            # continue\n",
    "        probs = []\n",
    "\n",
    "        for tag in tags:\n",
    "            # Transmission probability\n",
    "            p_trans = tag_triplet.get(prev2_tag + ' ' + prev_tag + ' ' + tag, 0)/ tag_pair.get(prev2_tag + ' ' + prev_tag, -1)\n",
    "            p_trans *= tag_pair.get(prev_tag + ' ' + tag, 0) / tag_freq.get(prev_tag, -1)\n",
    "            \n",
    "            if count_emiss:\n",
    "                # Emmission probability (only counted if the word is not OOV)\n",
    "                p_emiss = token_tag.get(\n",
    "                    word + \"/\" + tag, 0) / tag_freq.get(tag, -1)\n",
    "                probs.append(p_trans * p_emiss)\n",
    "            else:\n",
    "                probs.append(p_trans)\n",
    "\n",
    "        chosen_tag = tags[np.argmax(probs)]\n",
    "        prev2_tag = prev_tag\n",
    "        prev_tag = chosen_tag\n",
    "\n",
    "        chosen_tags.append(chosen_tag)\n",
    "\n",
    "    return chosen_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27813\n",
      "Number of Tags:  12\n",
      "{'ADP': 43660, 'DET': 42087, 'NOUN': 80317, 'VERB': 59540, 'ADJ': 24131, 'CONJ': 11867, 'PRT': 10219, '.': 47569, 'ADV': 18429, 'NUM': 4360, 'PRON': 18010, 'X': 398}\n",
      "Number of Token/Tag:  29593\n",
      "Number of Tag Tag:  140\n",
      "Number of Tag Tag Tag:  1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9164/9164 [00:04<00:00, 1888.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27702\n",
      "Number of Tags:  12\n",
      "{'VERB': 60045, 'ADJ': 24352, 'NOUN': 80458, 'DET': 42158, 'ADP': 43855, '.': 47682, 'CONJ': 11875, 'ADV': 18457, 'PRON': 18081, 'PRT': 10510, 'NUM': 4454, 'X': 398}\n",
      "Number of Token/Tag:  29491\n",
      "Number of Tag Tag:  141\n",
      "Number of Tag Tag Tag:  1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9164/9164 [00:06<00:00, 1376.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  27748\n",
      "Number of Tags:  12\n",
      "{'ADP': 44207, 'DET': 42083, 'NOUN': 80765, 'VERB': 59947, 'ADJ': 24449, 'CONJ': 11816, 'PRT': 10515, '.': 47859, 'ADV': 18480, 'NUM': 4514, 'PRON': 18367, 'X': 384}\n",
      "Number of Token/Tag:  29544\n",
      "Number of Tag Tag:  141\n",
      "Number of Tag Tag Tag:  1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9163/9163 [00:06<00:00, 1435.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average metrics: \n"
     ]
    }
   ],
   "source": [
    "q1b_metrics = KFoldHMM(data, create_params, viterbi)\n",
    "# kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# unique_tags = []\n",
    "# metrics = []\n",
    "# for train_index, test_index in kf.split(data):\n",
    "#     train_set, test_Xy = data[train_index], data[test_index]\n",
    "#     test_X, test_y = split_Xy(test_Xy)\n",
    "#     tags, preds = HMM(train_set, test_X)\n",
    "#     unique_tags = tags\n",
    "#     metrics.append(evaluate_HMM(tags, preds, test_y, to_print=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Metrics is an array of dictionaries with the following values:\n",
    "        \"accuracy\": Accuracy for all tags,\n",
    "        \"precision\": Precision for all tags\n",
    "        \"recall\": Recall for all tags\n",
    "        \"f1-score\": F1 score for all tags\n",
    "        \"cm\": Confusion Matrix\n",
    "        \"tagwise_precision\" : array of precision based on tag\n",
    "        \"tagwise_recall\": array of recall based on tag\n",
    "        \"tagwise_f1\": array of f1 scores based on tag,\n",
    "        \"model_name\": Display Name of the model,\n",
    "        \"model_df_name\": Name of the model to be stored in df columns \n",
    "    \"\"\"\n",
    "    num_models = len(metrics)\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\n",
    "            \"model_name\": [metrics[i][\"model_name\"] for i in range(num_models)],\n",
    "            \"accuracy\": [metrics[i][\"accuracy\"] for i in range(num_models)],\n",
    "            \"precision\": [metrics[i][\"precision\"] for i in range(num_models)],\n",
    "            \"recall\": [metrics[i][\"recall\"] for i in range(num_models)],\n",
    "            \"f1-score\": [metrics[i][\"f1-score\"] for i in range(num_models)]\n",
    "        }\n",
    "    )\n",
    "    tagwise_metric_df = pd.DataFrame(\n",
    "        {\n",
    "            \"tag\": unique_tags\n",
    "            # \"precision\": [metrics[i][\"tagwise_precision\"] for i in range(num_models)],\n",
    "            # \"recall\": [metrics[i][\"tagwise_recall\"] for i in range(num_models)],\n",
    "            # \"f1-score\": [metrics[i][\"tagwise_f1\"] for i in range(num_models)]\n",
    "        }\n",
    "    )\n",
    "    for i in range(len(metrics)):\n",
    "        tagwise_metric_df[\"precision_\" + metrics[i][\"model_df_name\"]] = metrics[i][\"tagwise_precision\"]\n",
    "    for i in range(len(metrics)):\n",
    "        tagwise_metric_df[\"recall_\" + metrics[i][\"model_df_name\"]] = metrics[i][\"tagwise_recall\"]\n",
    "    for i in range(len(metrics)):\n",
    "        tagwise_metric_df[\"f1_\" + metrics[i][\"model_df_name\"]] = metrics[i][\"tagwise_f1\"]\n",
    "    return metric_df, tagwise_metric_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1a_metrics['model_name'] = \"HMM\"\n",
    "q1a_metrics['model_df_name'] = \"HMM\"\n",
    "q1b_metrics['model_name'] = \"HMM Optional\"\n",
    "q1b_metrics['model_df_name'] = \"HMMb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df, tagwise_metric_df = compare_metrics([q1a_metrics, q1b_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HMM</td>\n",
       "      <td>0.934288</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.934288</td>\n",
       "      <td>0.934629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HMM Optional</td>\n",
       "      <td>0.925617</td>\n",
       "      <td>0.925731</td>\n",
       "      <td>0.925617</td>\n",
       "      <td>0.925674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  accuracy  precision    recall  f1-score\n",
       "0           HMM  0.934288   0.934971  0.934288  0.934629\n",
       "1  HMM Optional  0.925617   0.925731  0.925617  0.925674"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision_HMM</th>\n",
       "      <th>precision_HMMb</th>\n",
       "      <th>recall_HMM</th>\n",
       "      <th>recall_HMMb</th>\n",
       "      <th>f1_HMM</th>\n",
       "      <th>f1_HMMb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADP</td>\n",
       "      <td>0.927881</td>\n",
       "      <td>0.885120</td>\n",
       "      <td>0.934325</td>\n",
       "      <td>0.950680</td>\n",
       "      <td>0.931088</td>\n",
       "      <td>0.915364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DET</td>\n",
       "      <td>0.932602</td>\n",
       "      <td>0.926729</td>\n",
       "      <td>0.936486</td>\n",
       "      <td>0.926712</td>\n",
       "      <td>0.933730</td>\n",
       "      <td>0.924853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.952553</td>\n",
       "      <td>0.916841</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.938739</td>\n",
       "      <td>0.937121</td>\n",
       "      <td>0.927659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VERB</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.950318</td>\n",
       "      <td>0.957994</td>\n",
       "      <td>0.945191</td>\n",
       "      <td>0.952887</td>\n",
       "      <td>0.947181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.916944</td>\n",
       "      <td>0.900266</td>\n",
       "      <td>0.874352</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.894771</td>\n",
       "      <td>0.878019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CONJ</td>\n",
       "      <td>0.968926</td>\n",
       "      <td>0.977792</td>\n",
       "      <td>0.996005</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.981933</td>\n",
       "      <td>0.984830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PRT</td>\n",
       "      <td>0.825178</td>\n",
       "      <td>0.870711</td>\n",
       "      <td>0.866165</td>\n",
       "      <td>0.667837</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.746073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>0.918396</td>\n",
       "      <td>0.940170</td>\n",
       "      <td>0.948445</td>\n",
       "      <td>0.940852</td>\n",
       "      <td>0.931809</td>\n",
       "      <td>0.939346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ADV</td>\n",
       "      <td>0.941688</td>\n",
       "      <td>0.935577</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>0.866358</td>\n",
       "      <td>0.910157</td>\n",
       "      <td>0.899586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NUM</td>\n",
       "      <td>0.875749</td>\n",
       "      <td>0.917480</td>\n",
       "      <td>0.871675</td>\n",
       "      <td>0.708741</td>\n",
       "      <td>0.872721</td>\n",
       "      <td>0.797294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PRON</td>\n",
       "      <td>0.984172</td>\n",
       "      <td>0.989887</td>\n",
       "      <td>0.924222</td>\n",
       "      <td>0.887790</td>\n",
       "      <td>0.953145</td>\n",
       "      <td>0.934291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>X</td>\n",
       "      <td>0.737916</td>\n",
       "      <td>0.746389</td>\n",
       "      <td>0.291296</td>\n",
       "      <td>0.198726</td>\n",
       "      <td>0.414717</td>\n",
       "      <td>0.312045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tag  precision_HMM  precision_HMMb  recall_HMM  recall_HMMb    f1_HMM  \\\n",
       "0    ADP       0.927881        0.885120    0.934325     0.950680  0.931088   \n",
       "1    DET       0.932602        0.926729    0.936486     0.926712  0.933730   \n",
       "2   NOUN       0.952553        0.916841    0.922200     0.938739  0.937121   \n",
       "3   VERB       0.948148        0.950318    0.957994     0.945191  0.952887   \n",
       "4    ADJ       0.916944        0.900266    0.874352     0.862799  0.894771   \n",
       "5   CONJ       0.968926        0.977792    0.996005     0.992252  0.981933   \n",
       "6    PRT       0.825178        0.870711    0.866165     0.667837  0.844828   \n",
       "7      .       0.918396        0.940170    0.948445     0.940852  0.931809   \n",
       "8    ADV       0.941688        0.935577    0.880705     0.866358  0.910157   \n",
       "9    NUM       0.875749        0.917480    0.871675     0.708741  0.872721   \n",
       "10  PRON       0.984172        0.989887    0.924222     0.887790  0.953145   \n",
       "11     X       0.737916        0.746389    0.291296     0.198726  0.414717   \n",
       "\n",
       "     f1_HMMb  \n",
       "0   0.915364  \n",
       "1   0.924853  \n",
       "2   0.927659  \n",
       "3   0.947181  \n",
       "4   0.878019  \n",
       "5   0.984830  \n",
       "6   0.746073  \n",
       "7   0.939346  \n",
       "8   0.899586  \n",
       "9   0.797294  \n",
       "10  0.934291  \n",
       "11  0.312045  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagwise_metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HMM_metrics.pkl', 'wb') as f:\n",
    "    import pickle\n",
    "    HMM_metrics = [q1a_metrics, q1b_metrics]\n",
    "    pickle.dump(HMM_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_tags.pkl', \"wb\") as f:\n",
    "    import pickle\n",
    "    pickle.dump(unique_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "220f31919fc022d292a5f800c84cebb2982377f1f20223b2735d9f41a3ba9d0f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
