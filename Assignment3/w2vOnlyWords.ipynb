{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingPath = './../data/GoogleNews-vectors-negative300.bin'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddingPath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './../data/Brown_tagged_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy(test_Xy):\n",
    "    \"\"\"\n",
    "    test_Xy: List of list of tokens and tags\n",
    "    Returns: List of tokens and list of tags\n",
    "    \"\"\"\n",
    "    test_y = []\n",
    "    test_X = []\n",
    "\n",
    "    for sent in test_Xy:\n",
    "        tagged_sent = sent.split(' ')\n",
    "        sent_y = []\n",
    "        sent_X = []\n",
    "\n",
    "        for word in tagged_sent:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            actual_word, tag = split_tag_word(word)\n",
    "            sent_X.append(actual_word)\n",
    "            sent_y.append(tag)\n",
    "\n",
    "        test_y.append(sent_y)\n",
    "        test_X.append(sent_X)\n",
    "\n",
    "    return test_X, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_word(inp):\n",
    "    \"\"\"\n",
    "    Returns word, tag for the given input\n",
    "    \"\"\"\n",
    "    arr = inp.split('/')\n",
    "    tag = arr[-1]\n",
    "    del arr[-1]\n",
    "    word = '/'.join(arr)\n",
    "    return word, tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testXy = split_Xy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(sentences, tags, window=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    unk = 0\n",
    "    for ctr in tqdm(range(len(sentences))):\n",
    "        sent = sentences[ctr]\n",
    "        for i in range(len(sent)):\n",
    "            vec, unknown = features_embs(sent, i, window)\n",
    "            X.append(vec)\n",
    "            y.append(tags[ctr][i])\n",
    "            unk += unknown\n",
    "    return X, y, unk\n",
    "\n",
    "\n",
    "def vectorize(trainSent, trainTags, window=1, embedding='word2vec'):\n",
    "    X, y, unk = transform_to_dataset(trainSent, trainTags, window)\n",
    "    # X = np.array(X)\n",
    "    # y = np.array(y)\n",
    "    print('Unknown words:', unk)\n",
    "    X = np.asarray(X, dtype=object)\n",
    "    y = np.asarray(y, dtype=object)\n",
    "    print('X shape:', X.shape)\n",
    "    print('y shape:', y.shape)\n",
    "    return X, y\n",
    "\n",
    "def getYtrain(sentences, tags):\n",
    "    y_train = []\n",
    "    for ctr in tqdm(range(len(sentences))):\n",
    "        sent = sentences[ctr]\n",
    "        for i in range(len(sent)):\n",
    "            y_train.append(tags[ctr][i])\n",
    "    y_train = np.array(y_train)\n",
    "    return y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27491/27491 [00:00<00:00, 114988.01it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train = getYtrain(trainSet, testXy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543149, 12)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27491/27491 [00:16<00:00, 1647.26it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "out_ = 0\n",
    "in_ = 0\n",
    "for i in tqdm(range(len(trainSet))):\n",
    "    for j in trainSet[i]:\n",
    "        try:\n",
    "            embs.append(embeddings[j])\n",
    "            in_ += 1\n",
    "        except:\n",
    "            oov = np.random.uniform(-0.25, 0.25, 300)\n",
    "            embs.append(oov)\n",
    "            out_ += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543149, (543149, 12))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs), y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous:  543149 (543149, 12)\n",
      "Current:  434519 108630 (434519, 12) (108630, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "embs_train1, embs_test1, y_train1, y_test1 = train_test_split(\n",
    "    embs, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Previous: \", len(embs), y_train.shape)\n",
    "print(\"Current: \", len(embs_train1), len(\n",
    "    embs_test1), y_train1.shape, y_test1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_train1 = np.array(embs_train1)\n",
    "embs_test1 = np.array(embs_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 22:24:39.091503: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 600)               180600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1812      \n",
      "=================================================================\n",
      "Total params: 407,862\n",
      "Trainable params: 407,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, activation='relu', input_dim=embs_train1.shape[1]))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(y_train1.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 22:25:14.946734: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-31 22:25:14.946757: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-31 22:25:14.949936: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-31 22:25:19.762063: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3395/3395 [==============================] - 51s 15ms/step - loss: 0.5268 - accuracy: 0.8351 - val_loss: 0.4821 - val_accuracy: 0.8486\n",
      "Epoch 2/7\n",
      "3395/3395 [==============================] - 37s 11ms/step - loss: 0.4761 - accuracy: 0.8471 - val_loss: 0.4768 - val_accuracy: 0.8487\n",
      "Epoch 3/7\n",
      "3395/3395 [==============================] - 44s 13ms/step - loss: 0.4637 - accuracy: 0.8495 - val_loss: 0.4724 - val_accuracy: 0.8497\n",
      "Epoch 4/7\n",
      "3395/3395 [==============================] - 43s 13ms/step - loss: 0.4525 - accuracy: 0.8508 - val_loss: 0.4768 - val_accuracy: 0.8508\n",
      "Epoch 5/7\n",
      "3395/3395 [==============================] - 43s 13ms/step - loss: 0.4361 - accuracy: 0.8523 - val_loss: 0.4907 - val_accuracy: 0.8496\n",
      "Epoch 6/7\n",
      "3395/3395 [==============================] - 43s 13ms/step - loss: 0.4115 - accuracy: 0.8554 - val_loss: 0.5151 - val_accuracy: 0.8481\n",
      "Epoch 7/7\n",
      "3395/3395 [==============================] - 42s 12ms/step - loss: 0.3785 - accuracy: 0.8622 - val_loss: 0.5622 - val_accuracy: 0.8408\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 7\n",
    "batch_size = 128\n",
    "cp = ModelCheckpoint(filepath=\"tagger.h5\",\n",
    "                     monitor='val_acc',\n",
    "                     save_best_only=True,\n",
    "                     verbose=1)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 write_graph=True,\n",
    "                 write_images=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "\n",
    "history = model.fit(embs_train1, y_train1,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1, validation_data=(embs_test1, y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
